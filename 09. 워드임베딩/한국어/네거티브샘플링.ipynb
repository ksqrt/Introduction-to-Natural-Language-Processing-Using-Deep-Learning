{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20뉴스그룹 데이터 전처리하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('총 샘플 수 :',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\",\n",
       " \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\",\n",
       " \"Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to continue this think tank charade of yours, your\\nfixation on Israel must stop.  You might have to start asking the\\nsame sort of questions of Arab countries as well.  You realize it\\nwould not work, as the Arab countries' treatment of Jews over the\\nlast several decades is so bad that your fixation on Israel would\\nbegin to look like the biased attack that it is.\\n\\n   Everyone in this group recognizes that your stupid 'Center for\\nPolicy Research' is nothing more than a fancy name for some bigot\\nwho hates Israel.\",\n",
       " 'Notwithstanding all the legitimate fuss about this proposal, how much\\nof a change is it?  ATT\\'s last product in this area (a) was priced over\\n$1000, as I suspect \\'clipper\\' phones will be; (b) came to the customer \\nwith the key automatically preregistered with government authorities. Thus,\\naside from attempting to further legitimize and solidify the fed\\'s posture,\\nClipper seems to be \"more of the same\", rather than a new direction.\\n   Yes, technology will eventually drive the cost down and thereby promote\\nmore widespread use- but at present, the man on the street is not going\\nto purchase a $1000 crypto telephone, especially when the guy on the other\\nend probably doesn\\'t have one anyway.  Am I missing something?\\n   The real question is what the gov will do in a year or two when air-\\ntight voice privacy on a phone line is as close as your nearest pc.  That\\nhas got to a problematic scenario for them, even if the extent of usage\\nnever surpasses the \\'underground\\' stature of PGP.',\n",
       " \"Well, I will have to change the scoring on my playoff pool.  Unfortunately\\nI don't have time right now, but I will certainly post the new scoring\\nrules by tomorrow.  Does it matter?  No, you'll enter anyway!!!  Good!\\n\\n--\\n    Keith Keller\\t\\t\\t\\tLET'S GO RANGERS!!!!!\\n\\t\\t\\t\\t\\t\\tLET'S GO QUAKERS!!!!!\\n\\tkkeller@mail.sas.upenn.edu\\t\\tIVY LEAGUE CHAMPS!!!!\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 샘플보기\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_924/367905128.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "# 1. 전처리 특수문자제거 +길이별 제거 + 소문자변환\n",
    "\n",
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 널값 확인하기\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10995\n"
     ]
    }
   ],
   "source": [
    "# 빈값을 null 값으로 변환하고 null 값 제거하기\n",
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.isnull().values.any()\n",
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수 :',len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 널값 확인하기\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well will have change scoring playoff pool unf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document                                          clean_doc\n",
       "0  Well i'm not sure about the story nad it did s...  well sure about story seem biased what disagre...\n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...  yeah expect people read actually accept hard a...\n",
       "2  Although I realize that principle is not one o...  although realize that principle your strongest...\n",
       "3  Notwithstanding all the legitimate fuss about ...  notwithstanding legitimate fuss about this pro...\n",
       "4  Well, I will have to change the scoring on my ...  well will have change scoring playoff pool unf..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어를 제거\n",
    "\n",
    "stop_words =stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x : x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 260, 353, 1651, 1839]\n",
      "[list(['well', 'sure', 'story', 'seem', 'biased', 'disagree', 'statement', 'media', 'ruin', 'israels', 'reputation', 'rediculous', 'media', 'israeli', 'media', 'world', 'lived', 'europe', 'realize', 'incidences', 'described', 'letter', 'occured', 'media', 'whole', 'seem', 'ignore', 'subsidizing', 'israels', 'existance', 'europeans', 'least', 'degree', 'think', 'might', 'reason', 'report', 'clearly', 'atrocities', 'shame', 'austria', 'daily', 'reports', 'inhuman', 'acts', 'commited', 'israeli', 'soldiers', 'blessing', 'received', 'government', 'makes', 'holocaust', 'guilt', 'away', 'look', 'jews', 'treating', 'races', 'power', 'unfortunate'])\n",
      " list(['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons'])\n",
      " list(['although', 'realize', 'principle', 'strongest', 'points', 'would', 'still', 'like', 'know', 'question', 'sort', 'arab', 'countries', 'want', 'continue', 'think', 'tank', 'charade', 'fixation', 'israel', 'must', 'stop', 'might', 'start', 'asking', 'sort', 'questions', 'arab', 'countries', 'well', 'realize', 'would', 'work', 'arab', 'countries', 'treatment', 'jews', 'last', 'several', 'decades', 'fixation', 'israel', 'would', 'begin', 'look', 'like', 'biased', 'attack', 'everyone', 'group', 'recognizes', 'stupid', 'center', 'policy', 'research', 'nothing', 'fancy', 'name', 'bigot', 'hates', 'israel'])\n",
      " list(['notwithstanding', 'legitimate', 'fuss', 'proposal', 'much', 'change', 'last', 'product', 'area', 'priced', 'suspect', 'clipper', 'phones', 'came', 'customer', 'automatically', 'preregistered', 'government', 'authorities', 'thus', 'aside', 'attempting', 'legitimize', 'solidify', 'posture', 'clipper', 'seems', 'rather', 'direction', 'technology', 'eventually', 'drive', 'cost', 'thereby', 'promote', 'widespread', 'present', 'street', 'going', 'purchase', 'crypto', 'telephone', 'especially', 'probably', 'anyway', 'missing', 'something', 'real', 'question', 'year', 'tight', 'voice', 'privacy', 'phone', 'line', 'close', 'nearest', 'problematic', 'scenario', 'even', 'extent', 'usage', 'never', 'surpasses', 'underground', 'stature'])\n",
      " list(['well', 'change', 'scoring', 'playoff', 'pool', 'unfortunately', 'time', 'right', 'certainly', 'post', 'scoring', 'rules', 'tomorrow', 'matter', 'enter', 'anyway', 'good', 'keith', 'keller', 'rangers', 'quakers', 'kkeller', 'mail', 'upenn', 'league', 'champs'])]\n",
      "총 샘플 수 : 10940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:5071: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index , sentence in enumerate(tokenized_doc) if len(sentence) <=1]\n",
    "print(drop_train[:5])\n",
    "\n",
    "tokenized_doc = np.delete(tokenized_doc, drop_train ,axis=0)\n",
    "print(tokenized_doc[:5])\n",
    "print(\"총 샘플 수 :\",len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 : 정수 인코딩\n",
    "tokenizer = Tokenizer()\n",
    "# 1.정수 인코딩을 만들고\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "# 2. 정수 인코딩을 적용 \n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {value : key for key, value in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['well', 'sure', 'story', 'seem', 'biased', 'disagree', 'statement', 'media', 'ruin', 'israels', 'reputation', 'rediculous', 'media', 'israeli', 'media', 'world', 'lived', 'europe', 'realize', 'incidences', 'described', 'letter', 'occured', 'media', 'whole', 'seem', 'ignore', 'subsidizing', 'israels', 'existance', 'europeans', 'least', 'degree', 'think', 'might', 'reason', 'report', 'clearly', 'atrocities', 'shame', 'austria', 'daily', 'reports', 'inhuman', 'acts', 'commited', 'israeli', 'soldiers', 'blessing', 'received', 'government', 'makes', 'holocaust', 'guilt', 'away', 'look', 'jews', 'treating', 'races', 'power', 'unfortunate'])\n",
      " list(['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons'])]\n",
      "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_doc[:2])\n",
    "print(encoded[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "print('단어 집합의 크기 : ',vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
